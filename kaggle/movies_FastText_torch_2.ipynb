{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变长输入的FastText实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = re.sub(r'<[^>]+>',' ',s)\n",
    "    s = re.sub(r'[^a-zA-Z\\']',' ',s)\n",
    "    s = s.lower()\n",
    "    s = s.split(\" \")\n",
    "    s = [w for w in s if not w in stop_words]\n",
    "    s_ngram = s\n",
    "    for ss in s:\n",
    "        nlis = list(ngrams(ss,3))\n",
    "        nlis = [tu[0]+tu[1]+tu[2] for tu in nlis]\n",
    "        s_ngram = s_ngram + nlis\n",
    "    return \" \".join(s_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Movie(data.Dataset):\n",
    "    def __init__(self, x, y=None, train=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.train = train\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.x[index],self.y[index]\n",
    "        return self.x[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "hidden_size = 32\n",
    "max_features = 10000  # 词汇表的大小\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FastText, self).__init__()\n",
    "        self.embd = nn.Embedding(max_features,embedding_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size,128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        res = t.Tensor()\n",
    "        for xi in x:\n",
    "            xi = self.embd(xi)\n",
    "            xi = t.mean(xi,0)\n",
    "            xi = self.fc(xi)\n",
    "            res = t.cat((res,xi))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, x, y, criterion, optimizer, save_model_path=r'model.pkl', batch_size=50,epoch=2, display_iter=10, num_threads=8):\n",
    "    t.set_num_threads(num_threads)\n",
    "    \n",
    "    for epoch_iter in range(epoch):\n",
    "        running_loss = 0.0\n",
    "        for i in range(500):\n",
    "\n",
    "            # 输入数据\n",
    "            inputs = x[i*batch_size:i*batch_size+batch_size]\n",
    "            labels = y[i*batch_size:i*batch_size+batch_size]\n",
    "            inputs = [t.Tensor(i).long() for i in inputs]\n",
    "            \n",
    "            labels = t.Tensor(labels).float()\n",
    "\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印log\n",
    "            running_loss += loss.item()\n",
    "            if i % display_iter == display_iter-1:\n",
    "                print('[%d,%5d] loss %.3f' % (epoch_iter+1, i+1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "    t.save(model.state_dict(), save_model_path) # 保存的是以字典 key - value pair 形式的数据，每一个参数对应着一个值 state_dict 状态字典 \n",
    "    print('training finished!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x, batch_size=50):\n",
    "    res = t.Tensor()\n",
    "    for i in range(500):\n",
    "        inputs = x[i*batch_size:i*batch_size+batch_size]\n",
    "        inputs = [t.Tensor(i).long() for i in inputs]\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        res = t.cat((res,outputs))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = r'E:\\kaggle\\movies'\n",
    "train_data_path = os.path.join(data_path,'labeledTrainData.tsv')\n",
    "test_data_path = os.path.join(data_path,'testData.tsv')\n",
    "train_df = pd.read_csv(train_data_path,header=0,sep='\\t')\n",
    "test_df = pd.read_csv(test_data_path,header=0,sep='\\t')\n",
    "test_df['text'] = test_df.review.apply(clean_text)\n",
    "train_df['text'] = train_df.review.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000 <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "train_x = list_tokenized_train\n",
    "test_x = list_tokenized_test\n",
    "train_y = np.array(train_df['sentiment'])\n",
    "print(len(train_x),len(test_x),type(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   10] loss 0.694\n",
      "[1,   20] loss 0.695\n",
      "[1,   30] loss 0.688\n",
      "[1,   40] loss 0.688\n",
      "[1,   50] loss 0.682\n",
      "[1,   60] loss 0.682\n",
      "[1,   70] loss 0.673\n",
      "[1,   80] loss 0.670\n",
      "[1,   90] loss 0.661\n",
      "[1,  100] loss 0.648\n",
      "[1,  110] loss 0.638\n",
      "[1,  120] loss 0.628\n",
      "[1,  130] loss 0.604\n",
      "[1,  140] loss 0.592\n",
      "[1,  150] loss 0.583\n",
      "[1,  160] loss 0.554\n",
      "[1,  170] loss 0.560\n",
      "[1,  180] loss 0.511\n",
      "[1,  190] loss 0.492\n",
      "[1,  200] loss 0.500\n",
      "[1,  210] loss 0.471\n",
      "[1,  220] loss 0.443\n",
      "[1,  230] loss 0.441\n",
      "[1,  240] loss 0.435\n",
      "[1,  250] loss 0.445\n",
      "[1,  260] loss 0.396\n",
      "[1,  270] loss 0.411\n",
      "[1,  280] loss 0.372\n",
      "[1,  290] loss 0.412\n",
      "[1,  300] loss 0.396\n",
      "[1,  310] loss 0.404\n",
      "[1,  320] loss 0.346\n",
      "[1,  330] loss 0.389\n",
      "[1,  340] loss 0.423\n",
      "[1,  350] loss 0.366\n",
      "[1,  360] loss 0.327\n",
      "[1,  370] loss 0.332\n",
      "[1,  380] loss 0.356\n",
      "[1,  390] loss 0.353\n",
      "[1,  400] loss 0.361\n",
      "[1,  410] loss 0.383\n",
      "[1,  420] loss 0.364\n",
      "[1,  430] loss 0.369\n",
      "[1,  440] loss 0.393\n",
      "[1,  450] loss 0.383\n",
      "[1,  460] loss 0.363\n",
      "[1,  470] loss 0.366\n",
      "[1,  480] loss 0.329\n",
      "[1,  490] loss 0.316\n",
      "[1,  500] loss 0.310\n",
      "[2,   10] loss 0.339\n",
      "[2,   20] loss 0.375\n",
      "[2,   30] loss 0.336\n",
      "[2,   40] loss 0.278\n",
      "[2,   50] loss 0.289\n",
      "[2,   60] loss 0.350\n",
      "[2,   70] loss 0.307\n",
      "[2,   80] loss 0.342\n",
      "[2,   90] loss 0.317\n",
      "[2,  100] loss 0.306\n",
      "[2,  110] loss 0.277\n",
      "[2,  120] loss 0.264\n",
      "[2,  130] loss 0.272\n",
      "[2,  140] loss 0.298\n",
      "[2,  150] loss 0.338\n",
      "[2,  160] loss 0.273\n",
      "[2,  170] loss 0.349\n",
      "[2,  180] loss 0.297\n",
      "[2,  190] loss 0.280\n",
      "[2,  200] loss 0.309\n",
      "[2,  210] loss 0.320\n",
      "[2,  220] loss 0.279\n",
      "[2,  230] loss 0.320\n",
      "[2,  240] loss 0.301\n",
      "[2,  250] loss 0.328\n",
      "[2,  260] loss 0.248\n",
      "[2,  270] loss 0.302\n",
      "[2,  280] loss 0.249\n",
      "[2,  290] loss 0.309\n",
      "[2,  300] loss 0.305\n",
      "[2,  310] loss 0.310\n",
      "[2,  320] loss 0.222\n",
      "[2,  330] loss 0.275\n",
      "[2,  340] loss 0.338\n",
      "[2,  350] loss 0.270\n",
      "[2,  360] loss 0.229\n",
      "[2,  370] loss 0.267\n",
      "[2,  380] loss 0.289\n",
      "[2,  390] loss 0.283\n",
      "[2,  400] loss 0.260\n",
      "[2,  410] loss 0.305\n",
      "[2,  420] loss 0.283\n",
      "[2,  430] loss 0.275\n",
      "[2,  440] loss 0.328\n",
      "[2,  450] loss 0.320\n",
      "[2,  460] loss 0.299\n",
      "[2,  470] loss 0.297\n",
      "[2,  480] loss 0.253\n",
      "[2,  490] loss 0.253\n",
      "[2,  500] loss 0.250\n",
      "training finished!!!!!\n"
     ]
    }
   ],
   "source": [
    "net = FastText()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = t.optim.Adam(net.parameters())\n",
    "epoch = 2\n",
    "batch_size = 50\n",
    "display_iter = 10\n",
    "num_threads = 8\n",
    "model_path = os.path.join(data_path,r'FastText.pkl')\n",
    "\n",
    "train(net, train_x, train_y, criterion=criterion, optimizer=optimizer, save_model_path=model_path,\n",
    "      epoch=epoch, display_iter=display_iter, num_threads=num_threads)\n",
    "result = predict(net, test_x, batch_size).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.array(result>0.5, dtype=np.int)\n",
    "FastText_df = pd.DataFrame({'id':test_df['id'],'sentiment':result})\n",
    "FastText_df.to_csv(os.path.join(data_path,'FastText_result_1.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
