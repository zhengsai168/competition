{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗文本&&生成字符级n-gram特征\n",
    " - 先过滤掉html标记，再除掉一些标点，只保留英文字母和单引号\n",
    " - 用nltk的ngrams接口对每个词生成字符级n-gram特征添加到文本末尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(s):\n",
    "    s = re.sub(r'<[^>]+>',' ',s)\n",
    "    s = re.sub(r'[^a-zA-Z\\']',' ',s)\n",
    "    s = s.lower()\n",
    "    s = s.split(\" \")\n",
    "    s = [w for w in s if not w in stop_words]\n",
    "    s_ngram = s\n",
    "    for ss in s:\n",
    "        nlis = list(ngrams(ss,3))\n",
    "        nlis = [tu[0]+tu[1]+tu[2] for tu in nlis]\n",
    "        s_ngram = s_ngram + nlis\n",
    "    return \" \".join(s_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造数据集\n",
    " - 根据train和predict分别生成含有label和不含label的dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Movie(data.Dataset):\n",
    "    def __init__(self, x, y=None, train=True):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.train = train\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.x[index],self.y[index]\n",
    "        return self.x[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 写FastText的结构\n",
    " - embedding层\n",
    " - 对词向量求均值\n",
    " - 全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "hidden_size = 32\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FastText, self).__init__()\n",
    "        self.embd = nn.Embedding(max_features,embedding_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size,128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.embd(x)\n",
    "        x = t.mean(x,1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer, save_model_path=r'model.pkl', epoch=2, display_iter=10, num_threads=8):\n",
    "    t.set_num_threads(num_threads)\n",
    "    \n",
    "    for epoch_iter in range(epoch):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # 输入数据\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.long()\n",
    "            labels = labels.float()\n",
    "\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印log\n",
    "            running_loss += loss.item()\n",
    "            if i % display_iter == display_iter-1:\n",
    "                print('[%d,%5d] loss %.3f' % (epoch_iter+1, i+1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "    t.save(model.state_dict(), save_model_path) # 保存的是以字典 key - value pair 形式的数据，每一个参数对应着一个值 state_dict 状态字典 \n",
    "    print('training finished!!!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分批预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, testloader):\n",
    "    res = t.Tensor()\n",
    "    for data in testloader:\n",
    "        res = t.cat((res, model(data.long()).squeeze()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = r'E:\\kaggle\\movies'\n",
    "train_data_path = os.path.join(data_path,'labeledTrainData.tsv')\n",
    "test_data_path = os.path.join(data_path,'testData.tsv')\n",
    "train_df = pd.read_csv(train_data_path,header=0,sep='\\t')\n",
    "test_df = pd.read_csv(test_data_path,header=0,sep='\\t')\n",
    "test_df['text'] = test_df.review.apply(clean_text)\n",
    "train_df['text'] = train_df.review.apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 清洗，将词转换为序号值，把每个文本弄成等长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 640) (25000, 640) (25000,)\n"
     ]
    }
   ],
   "source": [
    "max_features = 10000  # 词汇表的大小\n",
    "max_len = 640   # 每个文本的长度\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_df['text'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test_df['text'])\n",
    "train_x = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "test_x = pad_sequences(list_tokenized_test, maxlen=max_len)\n",
    "train_y = train_df['sentiment']\n",
    "print(train_x.shape,test_x.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数，优化器，超参数，train和predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   10] loss 0.695\n",
      "[1,   20] loss 0.707\n",
      "[1,   30] loss 0.694\n",
      "[1,   40] loss 0.695\n",
      "[1,   50] loss 0.694\n",
      "[1,   60] loss 0.691\n",
      "[1,   70] loss 0.693\n",
      "[1,   80] loss 0.690\n",
      "[1,   90] loss 0.693\n",
      "[1,  100] loss 0.694\n",
      "[1,  110] loss 0.689\n",
      "[1,  120] loss 0.685\n",
      "[1,  130] loss 0.682\n",
      "[1,  140] loss 0.679\n",
      "[1,  150] loss 0.682\n",
      "[1,  160] loss 0.675\n",
      "[1,  170] loss 0.673\n",
      "[1,  180] loss 0.673\n",
      "[1,  190] loss 0.668\n",
      "[1,  200] loss 0.670\n",
      "[1,  210] loss 0.660\n",
      "[1,  220] loss 0.655\n",
      "[1,  230] loss 0.644\n",
      "[1,  240] loss 0.643\n",
      "[1,  250] loss 0.632\n",
      "[1,  260] loss 0.625\n",
      "[1,  270] loss 0.601\n",
      "[1,  280] loss 0.606\n",
      "[1,  290] loss 0.596\n",
      "[1,  300] loss 0.586\n",
      "[1,  310] loss 0.557\n",
      "[1,  320] loss 0.558\n",
      "[1,  330] loss 0.539\n",
      "[1,  340] loss 0.548\n",
      "[1,  350] loss 0.520\n",
      "[1,  360] loss 0.506\n",
      "[1,  370] loss 0.533\n",
      "[1,  380] loss 0.489\n",
      "[1,  390] loss 0.497\n",
      "[1,  400] loss 0.487\n",
      "[1,  410] loss 0.481\n",
      "[1,  420] loss 0.476\n",
      "[1,  430] loss 0.457\n",
      "[1,  440] loss 0.456\n",
      "[1,  450] loss 0.427\n",
      "[1,  460] loss 0.399\n",
      "[1,  470] loss 0.426\n",
      "[1,  480] loss 0.409\n",
      "[1,  490] loss 0.427\n",
      "[1,  500] loss 0.468\n",
      "[2,   10] loss 0.403\n",
      "[2,   20] loss 0.379\n",
      "[2,   30] loss 0.396\n",
      "[2,   40] loss 0.422\n",
      "[2,   50] loss 0.377\n",
      "[2,   60] loss 0.363\n",
      "[2,   70] loss 0.406\n",
      "[2,   80] loss 0.407\n",
      "[2,   90] loss 0.363\n",
      "[2,  100] loss 0.369\n",
      "[2,  110] loss 0.336\n",
      "[2,  120] loss 0.336\n",
      "[2,  130] loss 0.381\n",
      "[2,  140] loss 0.401\n",
      "[2,  150] loss 0.348\n",
      "[2,  160] loss 0.386\n",
      "[2,  170] loss 0.358\n",
      "[2,  180] loss 0.367\n",
      "[2,  190] loss 0.379\n",
      "[2,  200] loss 0.373\n",
      "[2,  210] loss 0.355\n",
      "[2,  220] loss 0.348\n",
      "[2,  230] loss 0.377\n",
      "[2,  240] loss 0.368\n",
      "[2,  250] loss 0.370\n",
      "[2,  260] loss 0.365\n",
      "[2,  270] loss 0.355\n",
      "[2,  280] loss 0.362\n",
      "[2,  290] loss 0.365\n",
      "[2,  300] loss 0.351\n",
      "[2,  310] loss 0.367\n",
      "[2,  320] loss 0.354\n",
      "[2,  330] loss 0.325\n",
      "[2,  340] loss 0.384\n",
      "[2,  350] loss 0.328\n",
      "[2,  360] loss 0.398\n",
      "[2,  370] loss 0.336\n",
      "[2,  380] loss 0.333\n",
      "[2,  390] loss 0.364\n",
      "[2,  400] loss 0.336\n",
      "[2,  410] loss 0.364\n",
      "[2,  420] loss 0.308\n",
      "[2,  430] loss 0.382\n",
      "[2,  440] loss 0.343\n",
      "[2,  450] loss 0.343\n",
      "[2,  460] loss 0.302\n",
      "[2,  470] loss 0.291\n",
      "[2,  480] loss 0.363\n",
      "[2,  490] loss 0.333\n",
      "[2,  500] loss 0.347\n",
      "training finished!!!!!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Movie(train_x, train_y)\n",
    "test_dataset = Movie(test_x,train=False)\n",
    "\n",
    "net = FastText()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = t.optim.Adam(net.parameters())\n",
    "epoch = 2\n",
    "batch_size = 50\n",
    "display_iter = 10\n",
    "num_threads = 8\n",
    "model_path = os.path.join(data_path,r'FastText.pkl')\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, drop_last=False)\n",
    "\n",
    "train(model=net, trainloader=trainloader, criterion=criterion, optimizer=optimizer, save_model_path=model_path, \\\n",
    "      epoch=epoch, display_iter=display_iter, num_threads=num_threads)\n",
    "result = predict(net, testloader).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出到csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.array(result>0.5, dtype=np.int)\n",
    "FastText_df = pd.DataFrame({'id':test_df['id'],'sentiment':result})\n",
    "FastText_df.to_csv(os.path.join(data_path,'FastText_result_1.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
