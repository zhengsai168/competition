{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from nltk.corpus import stopwords\n",
    "import torch as t\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = r'E:\\kaggle\\movies'\n",
    "train_data_path = os.path.join(data_path,'labeledTrainData.tsv')\n",
    "test_data_path = os.path.join(data_path,'testData.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'pos',\n",
       " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the imdb training dataset\n",
    "train = imdb_dataset(train=True)\n",
    "train[0]  # RETURNS: {'text': 'For a movie that gets..', 'sentiment': 'pos'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    25000\n",
       "text         25000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_imdb = pd.DataFrame(list(train))\n",
    "df_imdb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_imdb['sentiment'] = df_imdb['sentiment'].map({'pos':1,'neg':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_imdb['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  Bromwell High is a cartoon comedy. It ran at t...\n",
       "1          1  Homelessness (or Houselessness as George Carli...\n",
       "2          1  Brilliant over-acting by Lesley Ann Warren. Be...\n",
       "3          1  This is easily the most underrated film inn th...\n",
       "4          1  This is not the typical Mel Brooks film. It wa..."
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = df_imdb['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 "
     ]
    }
   ],
   "source": [
    "def prossed(i,tol):\n",
    "    if(i%(tol//100)==0):\n",
    "        print(i//(tol//100),end=' ')\n",
    "i = 0\n",
    "stopw = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\'\\s]','',text)\n",
    "    text = text.lower()\n",
    "    text = text.split(' ')\n",
    "    text = [word for word in text if not word in stopw]\n",
    "    text = ' '.join(text)\n",
    "    global i\n",
    "    i = i+1\n",
    "    prossed(i,25000)\n",
    "    return text\n",
    "df_imdb['prossed_reviews'] = df_imdb.text.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df_imdb['prossed_reviews'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df_imdb['prossed_reviews'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[211, 941, 119, 1934, 9, 5661, 288, 41, 5191, 4856, 59, 4760, 5867, 357, 150, 1832, 13, 2202, 521, 5191, 1888, 5790, 1351, 12, 104, 1085, 113, 767, 2782, 5662, 542, 1351, 106, 269, 1335, 3445, 642, 3395, 288, 1055, 211, 238, 251, 2808, 54, 4, 5191, 1335, 2275, 211, 392, 30, 1324, 456, 26, 211, 131, 2136]\n",
      "len 58\n",
      "(25000, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list_tokenized_train[0])\n",
    "print('len',len(list_tokenized_train[0]))\n",
    "df_imdb.prossed_reviews.apply(lambda x: len(x.split(\" \"))).mean()\n",
    "maxlen = 125\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "print(X_t.shape)\n",
    "y = df_imdb['sentiment']\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 6000\n",
    "embed_size=300\n",
    "hidden_size=256\n",
    "class mvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mvNet,self).__init__()\n",
    "        self.embd = nn.Embedding(max_features,embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_size,128)\n",
    "        self.linear2 = nn.Linear(128,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.embd(x)\n",
    "        x,_ = self.lstm(x)\n",
    "        x = x[:,-1,:].squeeze()\n",
    "        m = nn.Sigmoid()\n",
    "        x = m(self.linear1(x))\n",
    "        x = m(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = mvNet()\n",
    "from torch import optim\n",
    "criterion = nn.BCELoss() # 交叉熵损失函数\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "t.save (net.state_dict(), os.path.join(data_path,'lstm.pk1')) # 保存的是以字典 key - value pair 形式的数据，每一个参数对应着一个值 state_dict 状态字典 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.695\n",
      "[1,    20] loss: 0.689\n",
      "[1,    30] loss: 0.674\n",
      "[1,    40] loss: 0.624\n",
      "[1,    50] loss: 0.674\n",
      "[1,    60] loss: 0.653\n",
      "[1,    70] loss: 0.639\n",
      "[1,    80] loss: 0.633\n",
      "[1,    90] loss: 0.563\n",
      "[1,   100] loss: 0.521\n",
      "[1,   110] loss: 0.514\n",
      "[1,   120] loss: 0.553\n",
      "[1,   130] loss: 0.518\n",
      "[1,   140] loss: 0.538\n",
      "[1,   150] loss: 0.519\n",
      "[1,   160] loss: 0.476\n",
      "[1,   170] loss: 0.507\n",
      "[1,   180] loss: 0.539\n",
      "[1,   190] loss: 0.515\n",
      "[1,   200] loss: 0.499\n",
      "[1,   210] loss: 0.554\n",
      "[1,   220] loss: 0.532\n",
      "[1,   230] loss: 0.492\n",
      "[1,   240] loss: 0.500\n",
      "[1,   250] loss: 0.500\n",
      "[1,   260] loss: 0.469\n",
      "[1,   270] loss: 0.447\n",
      "[1,   280] loss: 0.501\n",
      "[1,   290] loss: 0.444\n",
      "[1,   300] loss: 0.456\n",
      "[1,   310] loss: 0.490\n",
      "[1,   320] loss: 0.491\n",
      "[1,   330] loss: 0.460\n",
      "[1,   340] loss: 0.433\n",
      "[1,   350] loss: 0.462\n",
      "[1,   360] loss: 0.578\n",
      "[1,   370] loss: 0.534\n",
      "[1,   380] loss: 0.516\n",
      "[1,   390] loss: 0.489\n",
      "[1,   400] loss: 0.507\n",
      "[1,   410] loss: 0.448\n",
      "[1,   420] loss: 0.446\n",
      "[1,   430] loss: 0.471\n",
      "[1,   440] loss: 0.480\n",
      "[1,   450] loss: 0.493\n",
      "[1,   460] loss: 0.417\n",
      "[1,   470] loss: 0.391\n",
      "[1,   480] loss: 0.450\n",
      "[1,   490] loss: 0.427\n",
      "[1,   500] loss: 0.389\n",
      "[2,    10] loss: 0.371\n",
      "[2,    20] loss: 0.331\n",
      "[2,    30] loss: 0.381\n",
      "[2,    40] loss: 0.371\n",
      "[2,    50] loss: 0.340\n",
      "[2,    60] loss: 0.382\n",
      "[2,    70] loss: 0.348\n",
      "[2,    80] loss: 0.362\n",
      "[2,    90] loss: 0.371\n",
      "[2,   100] loss: 0.385\n",
      "[2,   110] loss: 0.341\n",
      "[2,   120] loss: 0.335\n",
      "[2,   130] loss: 0.360\n",
      "[2,   140] loss: 0.389\n",
      "[2,   150] loss: 0.379\n",
      "[2,   160] loss: 0.408\n",
      "[2,   170] loss: 0.559\n",
      "[2,   180] loss: 0.480\n",
      "[2,   190] loss: 0.426\n",
      "[2,   200] loss: 0.493\n",
      "[2,   210] loss: 0.458\n",
      "[2,   220] loss: 0.394\n",
      "[2,   230] loss: 0.407\n",
      "[2,   240] loss: 0.357\n",
      "[2,   250] loss: 0.424\n",
      "[2,   260] loss: 0.401\n",
      "[2,   270] loss: 0.341\n",
      "[2,   280] loss: 0.463\n",
      "[2,   290] loss: 0.387\n",
      "[2,   300] loss: 0.434\n",
      "[2,   310] loss: 0.368\n",
      "[2,   320] loss: 0.371\n",
      "[2,   330] loss: 0.400\n",
      "[2,   340] loss: 0.375\n",
      "[2,   350] loss: 0.370\n",
      "[2,   360] loss: 0.378\n",
      "[2,   370] loss: 0.401\n",
      "[2,   380] loss: 0.297\n",
      "[2,   390] loss: 0.368\n",
      "[2,   400] loss: 0.355\n",
      "[2,   410] loss: 0.315\n",
      "[2,   420] loss: 0.336\n",
      "[2,   430] loss: 0.307\n",
      "[2,   440] loss: 0.327\n",
      "[2,   450] loss: 0.328\n",
      "[2,   460] loss: 0.378\n",
      "[2,   470] loss: 0.351\n",
      "[2,   480] loss: 0.331\n",
      "[2,   490] loss: 0.395\n",
      "[2,   500] loss: 0.353\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "class Movie(data.Dataset):\n",
    "    def __init__(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, index):\n",
    "        data = self.x[index]\n",
    "        label = self.y[index]\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "movdata = Movie(X_t,y)\n",
    "trainloader = DataLoader(movdata,batch_size=50, shuffle=True, num_workers=0, drop_last=False)\n",
    "\n",
    "t.set_num_threads(8)\n",
    "for epoch in range(2):  \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # 输入数据\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.float()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   \n",
    "        \n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9: # 每2000个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "# 模型参数保存\n",
    "t.save (net.state_dict(), os.path.join(data_path,'lstm.pk1')) # 保存的是以字典 key - value pair 形式的数据，每一个参数对应着一个值 state_dict 状态字典 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.255\n",
      "[1,    20] loss: 0.251\n",
      "[1,    30] loss: 0.287\n",
      "[1,    40] loss: 0.311\n",
      "[1,    50] loss: 0.300\n",
      "[1,    60] loss: 0.316\n",
      "[1,    70] loss: 0.297\n",
      "[1,    80] loss: 0.328\n",
      "[1,    90] loss: 0.303\n",
      "[1,   100] loss: 0.284\n",
      "[1,   110] loss: 0.279\n",
      "[1,   120] loss: 0.286\n",
      "[1,   130] loss: 0.323\n",
      "[1,   140] loss: 0.306\n",
      "[1,   150] loss: 0.296\n",
      "[1,   160] loss: 0.280\n",
      "[1,   170] loss: 0.260\n",
      "[1,   180] loss: 0.288\n",
      "[1,   190] loss: 0.268\n",
      "[1,   200] loss: 0.276\n",
      "[1,   210] loss: 0.266\n",
      "[1,   220] loss: 0.320\n",
      "[1,   230] loss: 0.278\n",
      "[1,   240] loss: 0.263\n",
      "[1,   250] loss: 0.308\n",
      "[1,   260] loss: 0.279\n",
      "[1,   270] loss: 0.262\n",
      "[1,   280] loss: 0.281\n",
      "[1,   290] loss: 0.285\n",
      "[1,   300] loss: 0.276\n",
      "[1,   310] loss: 0.240\n",
      "[1,   320] loss: 0.264\n",
      "[1,   330] loss: 0.293\n",
      "[1,   340] loss: 0.251\n",
      "[1,   350] loss: 0.281\n",
      "[1,   360] loss: 0.260\n",
      "[1,   370] loss: 0.269\n",
      "[1,   380] loss: 0.272\n",
      "[1,   390] loss: 0.284\n",
      "[1,   400] loss: 0.292\n",
      "[1,   410] loss: 0.266\n",
      "[1,   420] loss: 0.233\n",
      "[1,   430] loss: 0.271\n",
      "[1,   440] loss: 0.286\n",
      "[1,   450] loss: 0.313\n",
      "[1,   460] loss: 0.274\n",
      "[1,   470] loss: 0.268\n",
      "[1,   480] loss: 0.233\n",
      "[1,   490] loss: 0.235\n",
      "[1,   500] loss: 0.290\n",
      "[2,    10] loss: 0.272\n",
      "[2,    20] loss: 0.271\n",
      "[2,    30] loss: 0.270\n",
      "[2,    40] loss: 0.267\n",
      "[2,    50] loss: 0.270\n",
      "[2,    60] loss: 0.270\n",
      "[2,    70] loss: 0.261\n",
      "[2,    80] loss: 0.282\n",
      "[2,    90] loss: 0.291\n",
      "[2,   100] loss: 0.255\n",
      "[2,   110] loss: 0.326\n",
      "[2,   120] loss: 0.316\n",
      "[2,   130] loss: 0.301\n",
      "[2,   140] loss: 0.277\n",
      "[2,   150] loss: 0.259\n",
      "[2,   160] loss: 0.262\n",
      "[2,   170] loss: 0.283\n",
      "[2,   180] loss: 0.272\n",
      "[2,   190] loss: 0.248\n",
      "[2,   200] loss: 0.286\n",
      "[2,   210] loss: 0.251\n",
      "[2,   220] loss: 0.221\n",
      "[2,   230] loss: 0.242\n",
      "[2,   240] loss: 0.275\n",
      "[2,   250] loss: 0.257\n",
      "[2,   260] loss: 0.257\n",
      "[2,   270] loss: 0.251\n",
      "[2,   280] loss: 0.368\n",
      "[2,   290] loss: 0.258\n",
      "[2,   300] loss: 0.258\n",
      "[2,   310] loss: 0.245\n",
      "[2,   320] loss: 0.273\n",
      "[2,   330] loss: 0.287\n",
      "[2,   340] loss: 0.263\n",
      "[2,   350] loss: 0.240\n",
      "[2,   360] loss: 0.250\n",
      "[2,   370] loss: 0.245\n",
      "[2,   380] loss: 0.275\n",
      "[2,   390] loss: 0.294\n",
      "[2,   400] loss: 0.245\n",
      "[2,   410] loss: 0.260\n",
      "[2,   420] loss: 0.263\n",
      "[2,   430] loss: 0.204\n",
      "[2,   440] loss: 0.273\n",
      "[2,   450] loss: 0.275\n",
      "[2,   460] loss: 0.273\n",
      "[2,   470] loss: 0.268\n",
      "[2,   480] loss: 0.235\n",
      "[2,   490] loss: 0.265\n",
      "[2,   500] loss: 0.289\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(t.load(os.path.join(data_path,'lstm.pk1')))\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "t.set_num_threads(8)\n",
    "for epoch in range(2):  \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # 输入数据\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.long()\n",
    "        labels = labels.float()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward \n",
    "        outputs = net(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()   \n",
    "        \n",
    "        # 更新参数 \n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印log信息\n",
    "        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9: # 每2000个batch打印一下训练状态\n",
    "            print('[%d, %5d] loss: %.3f' \\\n",
    "                  % (epoch+1, i+1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_df = pd.read_csv(os.path.join(data_path,\"testData.tsv\"), header=0, sep='\\t')\n",
    "test_df['pro_review'] = test_df.review.apply(clean_text)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(test_df['pro_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 125\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = np.array([])\n",
    "for i in range(250):\n",
    "    tmp = net(t.tensor(X_t[i*100:i*100+100]).long())\n",
    "    tmp = tmp.squeeze().data.numpy()\n",
    "    tmp = np.array(tmp>0.5,dtype=np.int)\n",
    "    y = np.concatenate((y,tmp))\n",
    "    \n",
    "lstm_df=pd.DataFrame({'id':test_df['id'],'sentiment':y})\n",
    "lstm_df.to_csv(os.path.join(data_path,\"lstm_result.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
